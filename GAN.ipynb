{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "682ceda1-0fa1-4a06-89b7-147fe797a45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:12<00:00, 19.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 50 with 12.28 s\n",
      "Generator loss: 2.13003122, Discriminator loss: 1.08465548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:11<00:00, 19.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 of 50 with 11.95 s\n",
      "Generator loss: 3.25853206, Discriminator loss: 0.79882521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:11<00:00, 19.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 of 50 with 11.86 s\n",
      "Generator loss: 2.26381321, Discriminator loss: 1.03289728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:11<00:00, 19.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 of 50 with 11.79 s\n",
      "Generator loss: 5.02727356, Discriminator loss: 0.95806929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:11<00:00, 20.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 of 50 with 11.67 s\n",
      "Generator loss: 1.54193485, Discriminator loss: 1.15454513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:12<00:00, 19.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 of 50 with 12.01 s\n",
      "Generator loss: 1.20995346, Discriminator loss: 1.21228088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:12<00:00, 19.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 of 50 with 12.16 s\n",
      "Generator loss: 1.88665737, Discriminator loss: 0.83374181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:11<00:00, 20.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 of 50 with 11.66 s\n",
      "Generator loss: 2.52689548, Discriminator loss: 0.76217125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:11<00:00, 20.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 of 50 with 11.73 s\n",
      "Generator loss: 3.09087994, Discriminator loss: 0.60641950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:11<00:00, 19.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 of 50 with 11.88 s\n",
      "Generator loss: 2.41959507, Discriminator loss: 0.63582949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:11<00:00, 20.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 of 50 with 11.50 s\n",
      "Generator loss: 2.79933195, Discriminator loss: 0.42930881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:11<00:00, 19.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 of 50 with 11.88 s\n",
      "Generator loss: 2.88485130, Discriminator loss: 0.55499313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:11<00:00, 19.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 of 50 with 11.81 s\n",
      "Generator loss: 2.63805512, Discriminator loss: 0.53504952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:11<00:00, 19.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 of 50 with 11.79 s\n",
      "Generator loss: 2.51752839, Discriminator loss: 0.64117173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:11<00:00, 19.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 of 50 with 11.89 s\n",
      "Generator loss: 2.62533147, Discriminator loss: 0.55121856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:11<00:00, 19.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 of 50 with 11.90 s\n",
      "Generator loss: 2.59575949, Discriminator loss: 0.62138507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:11<00:00, 19.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 of 50 with 11.83 s\n",
      "Generator loss: 3.01976066, Discriminator loss: 0.48891633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:11<00:00, 19.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 of 50 with 11.98 s\n",
      "Generator loss: 2.77189101, Discriminator loss: 0.55492506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:12<00:00, 19.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 of 50 with 12.10 s\n",
      "Generator loss: 2.62682214, Discriminator loss: 0.56405070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:11<00:00, 19.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 of 50 with 11.80 s\n",
      "Generator loss: 3.05919038, Discriminator loss: 0.55058092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:11<00:00, 19.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 of 50 with 11.98 s\n",
      "Generator loss: 2.82220148, Discriminator loss: 0.49172249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:11<00:00, 19.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 of 50 with 11.95 s\n",
      "Generator loss: 3.01892633, Discriminator loss: 0.46934201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:12<00:00, 19.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 of 50 with 12.11 s\n",
      "Generator loss: 2.46242728, Discriminator loss: 0.62859489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:12<00:00, 19.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 of 50 with 12.08 s\n",
      "Generator loss: 2.54905207, Discriminator loss: 0.59296365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:11<00:00, 20.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 of 50 with 11.66 s\n",
      "Generator loss: 2.54541469, Discriminator loss: 0.58360817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:12<00:00, 19.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 of 50 with 12.25 s\n",
      "Generator loss: 2.36031498, Discriminator loss: 0.63162709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:12<00:00, 19.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 of 50 with 12.04 s\n",
      "Generator loss: 2.48547286, Discriminator loss: 0.61072952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:11<00:00, 20.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 of 50 with 11.76 s\n",
      "Generator loss: 2.47518652, Discriminator loss: 0.63351383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:11<00:00, 19.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 of 50 with 11.88 s\n",
      "Generator loss: 2.43427240, Discriminator loss: 0.61507189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:11<00:00, 20.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 of 50 with 11.62 s\n",
      "Generator loss: 2.53115766, Discriminator loss: 0.61495362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:11<00:00, 20.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31 of 50 with 11.71 s\n",
      "Generator loss: 2.39311050, Discriminator loss: 0.63753537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:11<00:00, 20.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 of 50 with 11.60 s\n",
      "Generator loss: 2.43121773, Discriminator loss: 0.63407204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:11<00:00, 20.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33 of 50 with 11.54 s\n",
      "Generator loss: 2.27530244, Discriminator loss: 0.65865806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:11<00:00, 20.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34 of 50 with 11.59 s\n",
      "Generator loss: 2.51160493, Discriminator loss: 0.61392676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:11<00:00, 20.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35 of 50 with 11.52 s\n",
      "Generator loss: 2.35674927, Discriminator loss: 0.62927663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:11<00:00, 20.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36 of 50 with 11.48 s\n",
      "Generator loss: 2.36552587, Discriminator loss: 0.62717471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:11<00:00, 20.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37 of 50 with 11.56 s\n",
      "Generator loss: 2.15220213, Discriminator loss: 0.66677236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:11<00:00, 19.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38 of 50 with 11.98 s\n",
      "Generator loss: 2.09346484, Discriminator loss: 0.71669081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:11<00:00, 20.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39 of 50 with 11.53 s\n",
      "Generator loss: 2.10737166, Discriminator loss: 0.72410951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:11<00:00, 20.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40 of 50 with 11.67 s\n",
      "Generator loss: 1.99674011, Discriminator loss: 0.75093832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:11<00:00, 20.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41 of 50 with 11.66 s\n",
      "Generator loss: 2.12174105, Discriminator loss: 0.73641493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:11<00:00, 19.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42 of 50 with 12.00 s\n",
      "Generator loss: 2.10711469, Discriminator loss: 0.74573004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:11<00:00, 20.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43 of 50 with 11.44 s\n",
      "Generator loss: 2.10844531, Discriminator loss: 0.70655686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:11<00:00, 20.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44 of 50 with 11.59 s\n",
      "Generator loss: 2.19442133, Discriminator loss: 0.70456548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:11<00:00, 20.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45 of 50 with 11.66 s\n",
      "Generator loss: 2.03686439, Discriminator loss: 0.73611650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:11<00:00, 20.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46 of 50 with 11.65 s\n",
      "Generator loss: 2.02050362, Discriminator loss: 0.76868241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:11<00:00, 20.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47 of 50 with 11.35 s\n",
      "Generator loss: 1.97328225, Discriminator loss: 0.76440765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:11<00:00, 20.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48 of 50 with 11.43 s\n",
      "Generator loss: 1.96521156, Discriminator loss: 0.76661925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:11<00:00, 20.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 of 50 with 11.57 s\n",
      "Generator loss: 1.96379870, Discriminator loss: 0.76014150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:11<00:00, 20.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50 of 50 with 11.47 s\n",
      "Generator loss: 1.83736089, Discriminator loss: 0.80985167\n",
      "Avg per epoch ptime: 11.77, total 50 epochs ptime: 632.22\n",
      "Training finish!... save training results\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"Image generator\n",
    "    \n",
    "    Takes a noise vector as input and syntheses a single channel image accordingly\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dims, output_dims):\n",
    "        \"\"\"Init function\n",
    "        \n",
    "        Declare the network structure as indicated in CW2 Guidance\n",
    "        \n",
    "        Arguments:\n",
    "            input_dims {int} -- Dimension of input noise vector\n",
    "            output_dims {int} -- Dimension of the output vector (flatten image)\n",
    "        \"\"\"\n",
    "        super(Generator, self).__init__()\n",
    "        self.fc1 = nn.Sequential(nn.Linear(input_dims, 256), nn.LeakyReLU(0.2))\n",
    "        \n",
    "        self.fc2 = nn.Sequential(nn.Linear(256, 512), nn.LeakyReLU(0.2))\n",
    "        \n",
    "        self.fc3 = nn.Sequential(nn.Linear(512, 1024), nn.LeakyReLU(0.2))\n",
    "        # output hidden layer\n",
    "        self.fc4 = nn.Sequential(nn.Linear(1024, output_dims),nn.Tanh())\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward function\n",
    "        \n",
    "        Arguments:\n",
    "            x {Tensor} -- a batch of noise vectors in shape (<batch_size>x<input_dims>)\n",
    "        \n",
    "        Returns:\n",
    "            Tensor -- a batch of flatten image in shape (<batch_size>x<output_dims>)\n",
    "        \"\"\"\n",
    "        ###  TODO: modify to be consistent with the network structure\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"Image discriminator\n",
    "    \n",
    "    Takes a image as input and predict if it is real from the dataset or fake synthesised by the generator\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dims, output_dims=1):\n",
    "        \"\"\"Init function\n",
    "        \n",
    "        Declare the discriminator network structure as indicated in CW2 Guidance\n",
    "        \n",
    "        Arguments:\n",
    "            input_dims {int} -- Dimension of the flatten input images\n",
    "        \n",
    "        Keyword Arguments:\n",
    "            output_dims {int} -- Predicted probability (default: {1})\n",
    "        \"\"\"\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        ###  TODO: Change the architecture and value as CW2 Guidance required\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(input_dims, 1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.fc3 = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.fc4 = nn.Sequential(\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward function\n",
    "        \n",
    "        Arguments:\n",
    "            x {Tensor} -- a batch of 2D image in shape (<batch_size>xHxW)\n",
    "        \n",
    "        Returns:\n",
    "            Tensor -- predicted probabilities (<batch_size>)\n",
    "        \"\"\"\n",
    "        ###  TODO: modify to be consistent with the network structure\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def show_result(G_net, z_, num_epoch, show=False, save=False, path='result.png'):\n",
    "    \"\"\"Result visualisation\n",
    "    \n",
    "    Show and save the generated figures in the grid fashion\n",
    "    \n",
    "    Arguments:\n",
    "        G_net {[nn.Module]} -- The generator instant\n",
    "        z_ {[Tensor]} -- Input noise vectors\n",
    "        num_epoch {[int]} -- Indicate how many epoch has the generator been trained\n",
    "    \n",
    "    Keyword Arguments:\n",
    "        show {bool} -- If to display the images (default: {False})\n",
    "        save {bool} -- If to store the images (default: {False})\n",
    "        path {str} -- path to store the images (default: {'result.png'})\n",
    "    \"\"\"\n",
    "\n",
    "    ###  TODO: complete the rest of part\n",
    "    # hint: use plt.subplots to construct grid\n",
    "    # hint: use a 5*5 grid to show all images \n",
    "    # hint: use plt.imshow and plt.savefig to display and store the images\n",
    "    fig, axs = plt.subplots(5, 5, figsize=(5, 5))\n",
    "    fig.suptitle('Epoch #{}'.format(num_epoch))\n",
    "\n",
    "    # Generate fake images\n",
    "    for ax, image in zip(axs.flat, G_net(z_).cpu().detach().numpy()):\n",
    "        ax.imshow(image.reshape((28, 28)), cmap='gray')\n",
    "\n",
    "    # Save and show the plots\n",
    "    if save:\n",
    "        fig.savefig(path)\n",
    "        plt.close()\n",
    "    if show:\n",
    "        fig.show()\n",
    "\n",
    "\n",
    "def show_train_hist(hist, show=False, save=False, path='Train_hist.png'):\n",
    "    \"\"\"Loss tracker\n",
    "    \n",
    "    Plot the losses of generator and discriminator independently to see the trend\n",
    "    \n",
    "    Arguments:\n",
    "        hist {[dict]} -- Tracking variables\n",
    "    \n",
    "    Keyword Arguments:\n",
    "        show {bool} -- If to display the figure (default: {False})\n",
    "        save {bool} -- If to store the figure (default: {False})\n",
    "        path {str} -- path to store the figure (default: {'Train_hist.png'})\n",
    "    \"\"\"\n",
    "    x = range(len(hist['D_losses']))\n",
    "\n",
    "    y1 = hist['D_losses']\n",
    "    y2 = hist['G_losses']\n",
    "\n",
    "    plt.plot(x, y1, label='D_loss')\n",
    "    plt.plot(x, y2, label='G_loss')\n",
    "\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    plt.legend(loc=4)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save:\n",
    "        plt.savefig(path)\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "def create_noise(num, dim):\n",
    "    \"\"\"Noise constructor\n",
    "    \n",
    "    returns a tensor filled with random numbers from a standard normal distribution\n",
    "    \n",
    "    Arguments:\n",
    "        num {int} -- Number of vectors\n",
    "        dim {int} -- Dimension of vectors\n",
    "    \n",
    "    Returns:\n",
    "        [Tensor] -- the generated noise vector batch\n",
    "    \"\"\"\n",
    "    return torch.randn(num, dim)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # initialise the device for training, if gpu is available, device = 'cuda', else: device = 'cpu'\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    data_dir = './MNIST_data/'\n",
    "    save_dir = './MNIST_GAN_results/'\n",
    "    image_save_dir = './MNIST_GAN_results/results'\n",
    "\n",
    "    # create folder if not exist\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.mkdir(save_dir)\n",
    "    if not os.path.exists(image_save_dir):\n",
    "        os.mkdir(image_save_dir)\n",
    "\n",
    "    # training parameters\n",
    "    batch_size = 100\n",
    "    learning_rate = 0.0002\n",
    "    epochs = 100\n",
    "\n",
    "    # parameters for Models\n",
    "    image_size = 28\n",
    "    G_input_dim = 100\n",
    "    G_output_dim = image_size * image_size\n",
    "    D_input_dim = image_size * image_size\n",
    "    D_output_dim = 1\n",
    "\n",
    "    # construct the dataset and data loader\n",
    "    transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean=(0.5,), std=(0.5,))])\n",
    "    train_data = datasets.MNIST(root=data_dir, train=True, transform=transform, download=True)\n",
    "    train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # declare the generator and discriminator networks    \n",
    "    G_net = Generator(G_input_dim, G_output_dim).to(device)\n",
    "    D_net = Discriminator(D_input_dim, D_output_dim).to(device)\n",
    "\n",
    "    # Binary Cross Entropy Loss function\n",
    "    criterion = nn.BCELoss().to(device)\n",
    "\n",
    "    # Initialise the Optimizers\n",
    "    G_optimizer = torch.optim.Adam(G_net.parameters(), lr=learning_rate)\n",
    "    D_optimizer = torch.optim.Adam(D_net.parameters(), lr=learning_rate)\n",
    "\n",
    "    # tracking variables\n",
    "    train_hist = {}\n",
    "    train_hist['D_losses'] = []\n",
    "    train_hist['G_losses'] = []\n",
    "    train_hist['per_epoch_ptimes'] = []\n",
    "    train_hist['total_ptime'] = []\n",
    "\n",
    "    start_time = time.time()\n",
    "    # training loop\n",
    "    for epoch in range(epochs):\n",
    "        G_net.train()\n",
    "        D_net.train()\n",
    "        Loss_G = []\n",
    "        Loss_D = []\n",
    "        epoch_start_time = time.time()\n",
    "        for (image, _) in tqdm(train_loader):\n",
    "            image = image.to(device)\n",
    "            b_size = len(image)\n",
    "            # creat real and fake labels\n",
    "            real_label = torch.ones(b_size, 1).to(device)\n",
    "            fake_label = torch.zeros(b_size, 1).to(device)\n",
    "\n",
    "            # generate fake images \n",
    "            data_fake = G_net(create_noise(b_size, G_input_dim).to(device))\n",
    "            data_real = image.view(b_size, D_input_dim)\n",
    "\n",
    "            # --------train the discriminator network----------\n",
    "            # compute the loss for real and fake images\n",
    "            output_real = D_net(data_real)\n",
    "            output_fake = D_net(data_fake)\n",
    "            loss_real = criterion(output_real, real_label)\n",
    "            loss_fake = criterion(output_fake, fake_label)\n",
    "            loss_d = loss_real + loss_fake\n",
    "\n",
    "            # back propagation\n",
    "            D_optimizer.zero_grad()\n",
    "            loss_d.backward()\n",
    "            D_optimizer.step()\n",
    "\n",
    "            # -------- train the generator network-----------\n",
    "            data_fake = G_net(create_noise(b_size, G_input_dim).to(device))\n",
    "\n",
    "            # compute the loss for generator network\n",
    "            output_fake = D_net(data_fake)\n",
    "            loss_g = criterion(output_fake, real_label)\n",
    "\n",
    "            ## back propagation\n",
    "            G_optimizer.zero_grad()\n",
    "            loss_g.backward()\n",
    "            G_optimizer.step()\n",
    "\n",
    "            ## store the loss of each iter\n",
    "            Loss_D.append(loss_d.item())\n",
    "            Loss_G.append(loss_g.item())\n",
    "\n",
    "        epoch_loss_g = np.mean(Loss_G)  # mean generator loss for the epoch\n",
    "        epoch_loss_d = np.mean(Loss_D)  # mean discriminator loss for the epoch\n",
    "        epoch_end_time = time.time()\n",
    "        per_epoch_ptime = epoch_end_time - epoch_start_time\n",
    "\n",
    "        print(\"Epoch %d of %d with %.2f s\" % (epoch + 1, epochs, per_epoch_ptime))\n",
    "        print(\"Generator loss: %.8f, Discriminator loss: %.8f\" % (epoch_loss_g, epoch_loss_d))\n",
    "\n",
    "        path = image_save_dir + '/MNIST_GAN_' + str(epoch + 1) + '.png'\n",
    "        show_result(G_net, create_noise(25, 100).to(device), (epoch + 1), save=True, path=path)\n",
    "\n",
    "        # record the loss for every epoch\n",
    "        train_hist['G_losses'].append(epoch_loss_g)\n",
    "        train_hist['D_losses'].append(epoch_loss_d)\n",
    "        train_hist['per_epoch_ptimes'].append(per_epoch_ptime)\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_ptime = end_time - start_time\n",
    "    train_hist['total_ptime'].append(total_ptime)\n",
    "\n",
    "    print('Avg per epoch ptime: %.2f, total %d epochs ptime: %.2f' % (\n",
    "        np.mean(train_hist['per_epoch_ptimes']), epochs, total_ptime))\n",
    "    print(\"Training finish!... save training results\")\n",
    "    with open(save_dir + '/train_hist.pkl', 'wb') as f:\n",
    "        pickle.dump(train_hist, f)\n",
    "    show_train_hist(train_hist, save=True, path=save_dir + '/MNIST_GAN_train_hist.png')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7a6776-eab5-4a48-ac60-27b94d60ad92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
